import json
import logging

from app.models.session import SessionState, ChatStep
from app.models.chat import ChatRequest, ChatResponse, PromptInfo, RAGSource
from app.llm.registry import provider_registry
from app.llm.prompts import SYSTEM_PROMPT, SPEC_CLASSIFICATION_PROMPT
from app.llm.schemas import SPEC_CLASSIFICATION_SCHEMA

logger = logging.getLogger(__name__)

_SPEC_CHECK_CHOICES = [
    {"value": "resolved", "label": "è§£æ±ºã—ã¾ã—ãŸ"},
    {"value": "not_resolved", "label": "è§£æ±ºã—ã¦ã„ã¾ã›ã‚“"},
    {"value": "already_tried", "label": "ãã‚Œã¯è©¦ã—ã¾ã—ãŸ / çŸ¥ã£ã¦ã„ã¾ã™"},
]


async def handle_spec_check(session: SessionState, request: ChatRequest) -> ChatResponse:
    """SPEC_CHECK step handler.

    Phase 1 (spec_check_shown=False): LLM classification + show explanation.
    Phase 2 (spec_check_shown=True): Handle user's choice.
    """

    # â”€â”€ Phase 2: ãƒ¦ãƒ¼ã‚¶ãƒ¼é¸æŠå¾Œ â”€â”€
    if session.spec_check_shown:
        return _handle_user_choice(session, request)

    # â”€â”€ Phase 1: LLMåˆ†é¡ â”€â”€
    return await _classify_and_respond(session, request)


def _handle_user_choice(session: SessionState, request: ChatRequest) -> ChatResponse:
    """Phase 2: Process user's response to spec explanation."""
    action_value = request.action_value or ""

    if action_value == "resolved":
        session.current_step = ChatStep.DONE
        return ChatResponse(
            session_id=session.session_id,
            current_step=ChatStep.DONE.value,
            prompt=PromptInfo(
                type="text",
                message="ãŠå½¹ã«ç«‹ã¦ã¦è‰¯ã‹ã£ãŸã§ã™ï¼ä»–ã«ã”è³ªå•ãŒã‚ã‚Œã°ã€æ–°ã—ã„å•è¨ºã‚’é–‹å§‹ã—ã¦ãã ã•ã„ã€‚\nå®‰å…¨é‹è»¢ã‚’ãŠé¡˜ã„ã„ãŸã—ã¾ã™ã€‚",
            ),
        )

    # not_resolved / already_tried / free text â†’ DIAGNOSING
    if request.message and request.message.strip() and action_value not in ("not_resolved", "already_tried"):
        session.collected_symptoms.append(request.message.strip())

    session.current_step = ChatStep.DIAGNOSING
    from app.chat_flow.step_diagnosing import handle_diagnosing
    return ChatResponse(
        session_id=session.session_id,
        current_step=ChatStep.DIAGNOSING.value,
        prompt=PromptInfo(
            type="text",
            message="æ‰¿çŸ¥ã—ã¾ã—ãŸã€‚è©³ã—ãç—‡çŠ¶ã‚’ãŠä¼ºã„ã—ã¾ã™ã€‚",
        ),
    )


async def _classify_and_respond(session: SessionState, request: ChatRequest) -> ChatResponse:
    """Phase 1: Use LLM to classify whether symptom is spec behavior."""
    provider = provider_registry.get_active()
    if not provider or not provider.is_configured():
        return _fallthrough_to_diagnosing(session)

    # Build RAG context from spec_rag_sources
    rag_context = _build_rag_context(session.spec_rag_sources)

    prompt_text = SPEC_CLASSIFICATION_PROMPT.format(
        make=session.vehicle_make or "ä¸æ˜",
        model=session.vehicle_model or "ä¸æ˜",
        year=session.vehicle_year or "ä¸æ˜",
        symptom=session.symptom_text or "",
        rag_context=rag_context,
    )

    try:
        response = await provider.chat(
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": prompt_text},
            ],
            temperature=0.1,
            response_format={"type": "json_schema", "json_schema": SPEC_CLASSIFICATION_SCHEMA},
        )
        result = json.loads(response.content)
    except Exception as e:
        logger.error(f"Spec classification LLM call failed: {e}")
        return _fallthrough_to_diagnosing(session)

    is_spec = result.get("is_spec_behavior", False)
    confidence = result.get("confidence", "low")
    explanation = result.get("explanation", "")
    manual_ref = result.get("manual_reference", "")
    reasoning = result.get("reasoning", "")

    logger.info(f"Spec classification: is_spec={is_spec}, confidence={confidence}, reasoning={reasoning}")

    # Only show spec explanation when high confidence AND is_spec_behavior
    if is_spec and confidence == "high":
        session.spec_check_shown = True

        message = f"ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã‚’ç¢ºèªã—ãŸã¨ã“ã‚ã€ã“ã‚Œã¯ä»•æ§˜ï¼ˆæ­£å¸¸ãªå‹•ä½œï¼‰ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\n\n{explanation}"
        if manual_ref:
            message += f"\n\nğŸ“– å‚è€ƒ: {manual_ref}"
        message += "\n\nã“ã®èª¬æ˜ã§ç–‘å•ã¯è§£æ±ºã—ã¾ã—ãŸã‹ï¼Ÿ"

        rag_sources = [
            RAGSource(
                content=s.get("content", ""),
                page=s.get("page", 0),
                section=s.get("section", ""),
                score=s.get("score", 0.0),
            )
            for s in session.spec_rag_sources
        ]

        return ChatResponse(
            session_id=session.session_id,
            current_step=ChatStep.SPEC_CHECK.value,
            prompt=PromptInfo(
                type="single_choice",
                message=message,
                choices=_SPEC_CHECK_CHOICES,
            ),
            rag_sources=rag_sources,
        )

    # Low/medium confidence or not spec â†’ fall through
    return _fallthrough_to_diagnosing(session)


def _build_rag_context(sources: list[dict]) -> str:
    """Build RAG context string from spec_rag_sources."""
    if not sources:
        return "é–¢é€£ã™ã‚‹ãƒãƒ‹ãƒ¥ã‚¢ãƒ«æƒ…å ±ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚"

    parts = []
    for i, s in enumerate(sources, 1):
        content = s.get("content", "")
        section = s.get("section", "")
        page = s.get("page", 0)
        content_type = s.get("content_type", "")
        header = f"[{i}] {section} (p.{page}, {content_type})" if section else f"[{i}] p.{page} ({content_type})"
        parts.append(f"{header}\n{content}")
    return "\n\n".join(parts)


def _fallthrough_to_diagnosing(session: SessionState) -> ChatResponse:
    """Fall through to DIAGNOSING step."""
    session.current_step = ChatStep.DIAGNOSING
    from app.chat_flow.step_diagnosing import handle_diagnosing
    return ChatResponse(
        session_id=session.session_id,
        current_step=ChatStep.DIAGNOSING.value,
        prompt=PromptInfo(
            type="text",
            message="ç—‡çŠ¶ã«ã¤ã„ã¦è©³ã—ããŠä¼ºã„ã—ã¾ã™ã€‚",
        ),
    )

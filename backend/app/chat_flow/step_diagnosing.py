import json
import logging
import re

from app.models.session import SessionState, ChatStep
from app.models.chat import ChatRequest, ChatResponse, PromptInfo, RAGSource
from app.llm.registry import provider_registry
from app.llm.prompts import SYSTEM_PROMPT, DIAGNOSTIC_PROMPT
from app.llm.schemas import DIAGNOSTIC_SCHEMA
from app.services.rag_service import rag_service
from app.services.urgency_assessor import keyword_urgency_check

logger = logging.getLogger(__name__)


FALLBACK_QUESTIONS = [
    "ç—‡çŠ¶ãŒå‡ºã‚‹ã®ã¯èµ°è¡Œä¸­ã§ã™ã‹ï¼Ÿãã‚Œã¨ã‚‚åœè»Šã—ã¦ã„ã‚‹ã¨ãã§ã™ã‹ï¼Ÿ",
    "ç—‡çŠ¶ãŒå‡ºã‚‹é »åº¦ã¯ã©ã®ãã‚‰ã„ã§ã™ã‹ï¼Ÿï¼ˆæ¯å›ãƒ»ãŸã¾ã«ãƒ»ä¸€åº¦ã ã‘ãªã©ï¼‰",
    "ã‚¨ãƒ³ã‚¸ãƒ³ã‚’ã‹ã‘ãŸã¨ãã€ãƒ¡ãƒ¼ã‚¿ãƒ¼ãƒ‘ãƒãƒ«ã«è¦‹æ…£ã‚Œãªã„è¡¨ç¤ºã¯å‡ºã¦ã„ã¾ã™ã‹ï¼Ÿ",
    "æœ€è¿‘ã€è»Šã®ç‚¹æ¤œã‚„ä¿®ç†ã‚’ã•ã‚Œã¾ã—ãŸã‹ï¼Ÿ",
    "ç—‡çŠ¶ãŒå‡ºã‚‹ã¨ãã€ä½•ã‹ç‰¹åˆ¥ãªæ“ä½œã‚’ã—ã¦ã„ã¾ã™ã‹ï¼Ÿï¼ˆä¾‹ï¼šã‚¨ã‚¢ã‚³ãƒ³ã‚’ã¤ã‘ãŸã€å‚é“ã‚’èµ°ã£ãŸãªã©ï¼‰",
]

# Task 2: å¾…ã¡ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ¤œå‡ºãƒ‘ã‚¿ãƒ¼ãƒ³
_WAITING_PATTERN = re.compile(r"ã¾ã¨ã‚|æ•´ç†|ãŠå¾…ã¡|ç¢ºèª.{0,5}ã•ã›|å°‘ã€…", re.UNICODE)

# Tip 1: å€™è£œãƒ©ãƒ™ãƒ«è£œåŠ©è¾æ›¸ï¼ˆLLMãŒçŸ­ã™ãã‚‹å˜èªã‚’è¿”ã—ãŸã¨ãã«èª¬æ˜ä»˜ãã«å¤‰æ›ï¼‰
_CANDIDATE_HINTS: dict[str, str] = {
    "ãƒ–ãƒ¬ãƒ¼ã‚­ãƒ‘ãƒƒãƒ‰": "ãƒ–ãƒ¬ãƒ¼ã‚­ãƒ‘ãƒƒãƒ‰æ‘©è€—ï¼ˆã‚­ãƒ¼ã‚­ãƒ¼/é‡‘å±éŸ³ï¼‰",
    "ãƒ­ãƒ¼ã‚¿ãƒ¼": "ãƒ–ãƒ¬ãƒ¼ã‚­ãƒ­ãƒ¼ã‚¿ãƒ¼ï¼ˆæ“¦ã‚Œ/æŒ¯å‹•ï¼‰",
    "ãƒ–ãƒ¬ãƒ¼ã‚­ãƒ­ãƒ¼ã‚¿ãƒ¼": "ãƒ–ãƒ¬ãƒ¼ã‚­ãƒ­ãƒ¼ã‚¿ãƒ¼ï¼ˆæ“¦ã‚Œ/æŒ¯å‹•ï¼‰",
    "ã‚¿ã‚¤ãƒ¤": "ã‚¿ã‚¤ãƒ¤ç•°å¸¸ï¼ˆãƒ‘ãƒ³ã‚¯/åæ‘©è€—ï¼‰",
    "ãƒãƒƒãƒ†ãƒªãƒ¼": "ãƒãƒƒãƒ†ãƒªãƒ¼åŠ£åŒ–ï¼ˆå§‹å‹•ä¸è‰¯ï¼‰",
    "ã‚ªãƒ«ã‚¿ãƒãƒ¼ã‚¿ãƒ¼": "ã‚ªãƒ«ã‚¿ãƒãƒ¼ã‚¿ãƒ¼ï¼ˆç™ºé›»æ©Ÿï¼‰æ•…éšœ",
    "ãƒ™ãƒ«ãƒˆ": "ãƒ™ãƒ«ãƒˆé¡æå‚·ï¼ˆã‚®ãƒ¼ã‚®ãƒ¼éŸ³ï¼‰",
    "ã‚¨ãƒ³ã‚¸ãƒ³": "ã‚¨ãƒ³ã‚¸ãƒ³å†…éƒ¨ç•°å¸¸ï¼ˆæŒ¯å‹•/ç•°éŸ³ï¼‰",
    "ã‚µã‚¹ãƒšãƒ³ã‚·ãƒ§ãƒ³": "ã‚µã‚¹ãƒšãƒ³ã‚·ãƒ§ãƒ³ï¼ˆã‚´ãƒˆã‚´ãƒˆéŸ³ï¼‰",
    "ã‚·ãƒ§ãƒƒã‚¯": "ã‚·ãƒ§ãƒƒã‚¯ã‚¢ãƒ–ã‚½ãƒ¼ãƒãƒ¼åŠ£åŒ–",
    "ãƒ—ãƒ©ã‚°": "ã‚¹ãƒ‘ãƒ¼ã‚¯ãƒ—ãƒ©ã‚°ä¸è‰¯ï¼ˆç‚¹ç«ï¼‰",
    "ç‡ƒæ–™": "ç‡ƒæ–™ç³»çµ±ï¼ˆå‡ºåŠ›ä½ä¸‹ï¼‰",
    "å†·å´æ°´": "å†·å´æ°´ä¸è¶³ï¼ˆéç†±ï¼‰",
    "ã‚¯ãƒ¼ãƒ©ãƒ³ãƒˆ": "ã‚¯ãƒ¼ãƒ©ãƒ³ãƒˆæ¼ã‚Œï¼ˆéç†±ï¼‰",
    "ã‚ªã‚¤ãƒ«": "ã‚¨ãƒ³ã‚¸ãƒ³ã‚ªã‚¤ãƒ«ï¼ˆæ¼ã‚Œ/ä¸è¶³ï¼‰",
    "ãƒãƒ•ãƒ©ãƒ¼": "ãƒãƒ•ãƒ©ãƒ¼ç•°å¸¸ï¼ˆæ’æ°—éŸ³å¤‰åŒ–ï¼‰",
    "CVT": "CVTï¼ˆå¤‰é€Ÿæ©Ÿï¼‰ä¸å…·åˆ",
    "AT": "ATï¼ˆã‚ªãƒ¼ãƒˆãƒï¼‰ä¸å…·åˆ",
    "ã‚¯ãƒ©ãƒƒãƒ": "ã‚¯ãƒ©ãƒƒãƒæ‘©è€—ï¼ˆæ»‘ã‚Šï¼‰",
    "ãƒãƒ–": "ãƒãƒ–ãƒ™ã‚¢ãƒªãƒ³ã‚°ï¼ˆèµ°è¡Œç•°éŸ³ï¼‰",
    "ãƒ‘ãƒ¯ã‚¹ãƒ†": "ãƒ‘ãƒ¯ãƒ¼ã‚¹ãƒ†ã‚¢ãƒªãƒ³ã‚°ä¸å…·åˆ",
}


def _enrich_candidate_label(label: str) -> str:
    """çŸ­ã™ãã‚‹å€™è£œãƒ©ãƒ™ãƒ«ã‚’è£œåŠ©è¾æ›¸ã§èª¬æ˜ä»˜ãã«å¤‰æ›ã™ã‚‹ã€‚"""
    s = label.strip()
    # æ—¢ã«æ‹¬å¼§ä»˜ãã‹ååˆ†ãªé•·ã•ãªã‚‰å¤‰æ›ä¸è¦
    if "ï¼ˆ" in s or "(" in s or len(s) >= 12:
        return s
    for key, hint in _CANDIDATE_HINTS.items():
        if key in s:
            return hint
    return s


# A) ask_question / clarify_term ã®æœ«å°¾ã«å¿…ãšè¿½åŠ ã™ã‚‹ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé¸æŠè‚¢
_DEFAULT_TAIL: list[dict] = [
    {"value": "dont_know", "label": "ã‚ã‹ã‚‰ãªã„"},
    {"value": "free_input", "label": "âœï¸ è‡ªç”±å…¥åŠ›"},
]


def _append_default_choices(choices: list[str] | None) -> list[dict]:
    """LLM ãŒè¿”ã—ãŸ choices ã«ã€Œã‚ã‹ã‚‰ãªã„ã€ã€Œè‡ªç”±å…¥åŠ›ã€ã‚’æœ«å°¾è¿½åŠ ã™ã‚‹ï¼ˆé‡è¤‡é™¤å¤–ï¼‰ã€‚"""
    result: list[dict] = []
    if choices:
        result = [{"value": c, "label": _enrich_candidate_label(c)} for c in choices]
    existing_values = {d["value"] for d in result}
    for tail in _DEFAULT_TAIL:
        if tail["value"] not in existing_values:
            result.append(tail)
    return result


def _is_waiting_message(msg: str) -> bool:
    """True if message looks like a 'please wait' transition, not a real question."""
    if "ï¼Ÿ" in msg or "?" in msg:
        return False
    return bool(_WAITING_PATTERN.search(msg))


def _build_conversation_text(session: SessionState) -> str:
    """Build conversation history as text for the prompt."""
    lines = []
    for entry in session.conversation_history:
        role = "ãƒ¦ãƒ¼ã‚¶ãƒ¼" if entry["role"] == "user" else "ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ"
        lines.append(f"{role}: {entry['content']}")
    return "\n".join(lines) if lines else "(åˆå›å…¥åŠ›)"


def _normalize_question(text: str) -> str:
    """Normalize a question for duplicate comparison."""
    text = re.sub(r"[ï¼Ÿ?ã€‚ã€ï¼!.,\sã€€]+", "", text)
    return text.lower()


def _is_duplicate_question(message: str, last_questions: list[str]) -> bool:
    """Check if the LLM question is semantically a duplicate of a recent one."""
    norm_new = _normalize_question(message)
    if not norm_new:
        return False
    for prev in last_questions:
        norm_prev = _normalize_question(prev)
        if not norm_prev:
            continue
        if norm_new == norm_prev:
            return True
        shorter, longer = sorted([norm_new, norm_prev], key=len)
        if len(shorter) >= 4 and shorter in longer:
            return True
    return False


def _pick_fallback_question(session: SessionState) -> str | None:
    """Pick a fallback question that hasn't been asked yet."""
    for q in FALLBACK_QUESTIONS:
        if not _is_duplicate_question(q, session.last_questions):
            return q
    return None


async def _llm_call(provider, diagnostic_prompt: str) -> dict:
    """Call LLM with DIAGNOSTIC_SCHEMA and return parsed JSON."""
    response = await provider.chat(
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": diagnostic_prompt},
        ],
        temperature=0.3,
        response_format={"type": "json_schema", "json_schema": DIAGNOSTIC_SCHEMA},
    )
    return json.loads(response.content)


async def handle_diagnosing(session: SessionState, request: ChatRequest) -> ChatResponse:
    user_input = (request.message or "").strip()

    # Handle "resolved" action from provide_answer step
    if request.action == "resolved":
        if request.action_value == "yes":
            session.current_step = ChatStep.DONE
            return ChatResponse(
                session_id=session.session_id,
                current_step=ChatStep.DONE.value,
                prompt=PromptInfo(
                    type="text",
                    message="ãŠå½¹ã«ç«‹ã¦ã¦è‰¯ã‹ã£ãŸã§ã™ï¼ä»–ã«ã”è³ªå•ãŒã‚ã‚Œã°ã€æ–°ã—ã„å•è¨ºã‚’é–‹å§‹ã—ã¦ãã ã•ã„ã€‚\nå®‰å…¨é‹è»¢ã‚’ãŠé¡˜ã„ã„ãŸã—ã¾ã™ã€‚",
                ),
            )
        elif request.action_value == "no":
            session.current_step = ChatStep.URGENCY_CHECK
            from app.chat_flow.step_urgency import handle_urgency_check
            return await handle_urgency_check(session, request)
        elif request.action_value == "book":
            # ã€Œç‚¹æ¤œã‚’äºˆç´„ã™ã‚‹ã€ã‚’ç›´æ¥é¸æŠ
            session.current_step = ChatStep.RESERVATION
            from app.chat_flow.step_reservation import handle_reservation
            return await handle_reservation(session, request)
        else:
            # æƒ³å®šå¤–ã®å€¤ã¯ãƒ­ã‚°ã ã‘æ®‹ã—ã¦ç„¡è¦–ï¼ˆdiagnosis_candidates ã¯ sendMessage çµŒç”±ãªã®ã§é€šå¸¸ã“ã“ã«æ¥ãªã„ï¼‰
            logger.warning(f"Unexpected resolved value: {request.action_value!r}")
            return ChatResponse(
                session_id=session.session_id,
                current_step=ChatStep.DIAGNOSING.value,
                prompt=PromptInfo(type="text", message="ç—‡çŠ¶ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚"),
            )

    if not user_input:
        return ChatResponse(
            session_id=session.session_id,
            current_step=ChatStep.DIAGNOSING.value,
            prompt=PromptInfo(
                type="text",
                message="ç—‡çŠ¶ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚",
            ),
        )

    # ---------------------------------------------------------------
    # 1. Save user input FIRST
    # ---------------------------------------------------------------
    session.collected_symptoms.append(user_input)
    session.conversation_history.append({"role": "user", "content": user_input})
    session.diagnostic_turn += 1

    # 2. Keyword-based urgency check (fast path for critical)
    all_symptoms = " ".join(session.collected_symptoms)
    keyword_result = keyword_urgency_check(all_symptoms)
    if keyword_result and keyword_result["level"] == "critical":
        session.urgency_level = "critical"
        session.can_drive = False
        session.current_step = ChatStep.RESERVATION
        from app.chat_flow.step_reservation import handle_reservation
        return await handle_reservation(session, request)

    # 3. RAG search
    rag_context = "é–¢é€£ã™ã‚‹ãƒãƒ‹ãƒ¥ã‚¢ãƒ«æƒ…å ±ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚"
    rag_sources: list[RAGSource] = []
    try:
        results = await rag_service.query(
            symptom=all_symptoms,
            vehicle_id=session.vehicle_id,
            make=session.vehicle_make or "",
            model=session.vehicle_model or "",
            year=session.vehicle_year or 0,
        )
        if results["sources"]:
            rag_context = results["answer"]
            rag_sources = [
                RAGSource(
                    content=s["content"],
                    page=s["page"],
                    section=s["section"],
                    score=s["score"],
                )
                for s in results["sources"]
            ]
    except Exception as e:
        logger.warning(f"RAG query failed: {e}")

    # 4. Build prompt
    conversation_text = _build_conversation_text(session)
    diagnostic_prompt = DIAGNOSTIC_PROMPT.format(
        make=session.vehicle_make or "ä¸æ˜",
        model=session.vehicle_model or "ä¸æ˜",
        year=session.vehicle_year or "ä¸æ˜",
        conversation_history=conversation_text,
        rag_context=rag_context,
    )

    # 5. Force provide_answer if max turns reached
    if session.diagnostic_turn >= session.max_diagnostic_turns:
        diagnostic_prompt += "\n\nã€é‡è¦ã€‘å•è¨ºå›æ•°ã®ä¸Šé™ã«é”ã—ã¾ã—ãŸã€‚ã“ã‚Œã¾ã§ã®æƒ…å ±ã‚’ã‚‚ã¨ã« action: \"provide_answer\" ã§å›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚"

    # ---------------------------------------------------------------
    # Task 3: turn>=4 ã§ä¸€å›ã ã‘å€™è£œæç¤º / å€™è£œé¸æŠå¾Œã¯ provide_answer ã¸
    # ---------------------------------------------------------------
    candidates_just_triggered = False
    if session.diagnostic_turn >= 4 and not session.candidates_shown:
        session.candidates_shown = True
        candidates_just_triggered = True
        diagnostic_prompt += (
            "\n\nã€é‡è¦ã€‘ã“ã‚Œã¾ã§ã®å•è¨ºã‹ã‚‰è€ƒãˆã‚‰ã‚Œã‚‹åŸå› ã‚’4ã¤ã«çµã‚Šè¾¼ã‚“ã§ãã ã•ã„ã€‚"
            "action: \"ask_question\", "
            "message ã¯ã€ŒåŸå› ã¨ã—ã¦æœ€ã‚‚è¿‘ã„ã‚‚ã®ã¯ã©ã‚Œã§ã™ã‹ï¼Ÿã€ï¼ˆ30æ–‡å­—ä»¥å†…ãƒ»1æ–‡ï¼‰, "
            "choices ã«è€ƒãˆã‚‰ã‚Œã‚‹åŸå› ã‚’4å€‹ï¼ˆå„10æ–‡å­—ä»¥å†…ï¼‰ï¼‹ã€Œãã®ä»–ã€ã®è¨ˆ5å€‹ã‚’å¿…ãšè¨­å®šã—ã¦ãã ã•ã„ã€‚"
        )
    elif session.candidates_shown and not candidates_just_triggered:
        # å€™è£œé¸æŠå¾Œ â†’ ã™ãã«å›ç­”ã‚’å‡ºã™
        diagnostic_prompt += (
            f"\n\nã€é‡è¦ã€‘ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒåŸå› å€™è£œã€Œ{user_input}ã€ã‚’é¸æŠã—ã¾ã—ãŸã€‚"
            "ã“ã®å€™è£œã«åŸºã¥ã„ã¦ã™ãã« action: \"provide_answer\" ã§å…·ä½“çš„ãªå›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚"
        )

    # 6. Call LLM
    provider = provider_registry.get_active()
    if not provider or not provider.is_configured():
        return ChatResponse(
            session_id=session.session_id,
            current_step=ChatStep.DIAGNOSING.value,
            prompt=PromptInfo(
                type="text",
                message="LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚",
            ),
        )

    try:
        result = await _llm_call(provider, diagnostic_prompt)
    except (json.JSONDecodeError, Exception) as e:
        logger.error(f"LLM diagnostic call failed: {e}")
        fallback_msg = _pick_fallback_question(session)
        session.last_questions.append(fallback_msg)
        session.conversation_history.append({"role": "assistant", "content": fallback_msg})
        return ChatResponse(
            session_id=session.session_id,
            current_step=ChatStep.DIAGNOSING.value,
            prompt=PromptInfo(type="text", message=fallback_msg),
        )

    action = result.get("action", "ask_question")
    message = result.get("message", "")
    urgency_flag = result.get("urgency_flag", "none")
    reasoning = result.get("reasoning", "")
    choices = result.get("choices")
    can_drive_llm: bool | None = result.get("can_drive")  # True / False / None

    logger.info(f"Diagnostic action={action}, urgency={urgency_flag}, reasoning={reasoning}")

    # ---------------------------------------------------------------
    # Task 2: å¾…ã¡ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ¤œå‡º â†’ ãƒªãƒˆãƒ©ã‚¤ã—ã¦ provide_answer ã‚’å–å¾—
    # ---------------------------------------------------------------
    if action == "ask_question" and _is_waiting_message(message):
        logger.warning(f"Waiting message detected, retrying: {message!r}")
        retry_prompt = (
            diagnostic_prompt
            + "\n\nã€é‡è¦ã€‘ã€Œã¾ã¨ã‚ã¾ã™ã€ã€Œæ•´ç†ã—ã¾ã™ã€ãªã©ã®å¾…æ©Ÿãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯å‡ºã•ãªã„ã§ãã ã•ã„ã€‚"
            "ä»Šã™ã action: \"provide_answer\" ã§è¨ºæ–­çµæœã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚"
        )
        try:
            result = await _llm_call(provider, retry_prompt)
            action = result.get("action", "provide_answer")
            message = result.get("message", message)
            urgency_flag = result.get("urgency_flag", urgency_flag)
            choices = result.get("choices")
            can_drive_llm = result.get("can_drive", can_drive_llm)
        except Exception as e:
            logger.warning(f"Retry LLM call failed: {e}")
            action = "provide_answer"

    # 7. Check urgency_flag from LLM
    if urgency_flag in ("high", "critical"):
        session.urgency_level = urgency_flag
        # LLM ã® can_drive å„ªå…ˆã€‚None ãªã‚‰ urgency_flag ã§æ¨å®šï¼ˆcritical â†’ Falseï¼‰
        session.can_drive = can_drive_llm if can_drive_llm is not None else (urgency_flag != "critical")
        if urgency_flag == "critical":
            session.current_step = ChatStep.RESERVATION
            session.conversation_history.append({"role": "assistant", "content": message})
            from app.chat_flow.step_reservation import handle_reservation
            return await handle_reservation(session, request)

    # 8. Dispatch based on action
    if action == "escalate":
        session.urgency_level = urgency_flag if urgency_flag in ("high", "critical") else "high"
        session.can_drive = session.urgency_level != "critical"
        session.conversation_history.append({"role": "assistant", "content": message})
        session.current_step = ChatStep.RESERVATION
        from app.chat_flow.step_reservation import handle_reservation
        return await handle_reservation(session, request)

    if action == "provide_answer":
        session.rag_answer = message
        session.conversation_history.append({"role": "assistant", "content": message})

        # C) high/critical â†’ å¼·ã„è­¦å‘Š + äºˆç´„å°ç·šï¼ˆreservation_choiceï¼‰
        if urgency_flag in ("high", "critical"):
            # True ã®ã¨ãã ã‘è‡ªèµ°å¯ã€‚False ã‚‚ Noneï¼ˆä¸æ˜ï¼‰ã‚‚ â†’ è‡ªèµ°ç¦æ­¢æ‰±ã„
            effective_can_drive = can_drive_llm if can_drive_llm is True else False
            session.urgency_level = urgency_flag
            session.can_drive = effective_can_drive
            session.current_step = ChatStep.RESERVATION

            if not effective_can_drive:
                warning = (
                    "ğŸš¨ã€è‡ªèµ°ç¦æ­¢ã€‘ã™ãã«é‹è»¢ã‚’ä¸­æ­¢ã—ã€å®‰å…¨ãªå ´æ‰€ã«åœè»Šã—ã¦ãã ã•ã„ã€‚\n\n"
                    f"{message}\n\n"
                    "ãƒ­ãƒ¼ãƒ‰ã‚µãƒ¼ãƒ“ã‚¹ã¸ã®é€£çµ¡ã‚’å¼·ããŠå‹§ã‚ã—ã¾ã™ã€‚"
                )
                reservation_choices = [
                    {"value": "dispatch", "label": "ãƒ­ãƒ¼ãƒ‰ã‚µãƒ¼ãƒ“ã‚¹ã‚’å‘¼ã¶"},
                    {"value": "skip", "label": "ä»Šã¯äºˆç´„ã—ãªã„"},
                ]
            else:
                warning = (
                    "âš ï¸ã€æ—©æ€¥ãªç‚¹æ¤œæ¨å¥¨ã€‘ç„¡ç†ãªé‹è»¢ã¯é¿ã‘ã¦ãã ã•ã„ã€‚\n\n"
                    f"{message}\n\n"
                    "æ—©æ€¥ã«ãƒ‡ã‚£ãƒ¼ãƒ©ãƒ¼ã¾ãŸã¯æ•´å‚™å·¥å ´ã§ã®ç‚¹æ¤œã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚"
                )
                reservation_choices = [
                    {"value": "dispatch", "label": "ãƒ­ãƒ¼ãƒ‰ã‚µãƒ¼ãƒ“ã‚¹ã‚’å‘¼ã¶"},
                    {"value": "visit", "label": "ãƒ‡ã‚£ãƒ¼ãƒ©ãƒ¼ã«æŒã¡è¾¼ã‚€"},
                    {"value": "skip", "label": "ä»Šã¯äºˆç´„ã—ãªã„"},
                ]

            return ChatResponse(
                session_id=session.session_id,
                current_step=ChatStep.RESERVATION.value,
                prompt=PromptInfo(
                    type="reservation_choice",
                    message=warning,
                    choices=reservation_choices,
                    booking_type=session.booking_type,
                ),
                rag_sources=rag_sources,
            )

        # low/medium/none â†’ è§£æ±ºç¢ºèª + äºˆç´„ã¸ã®ã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆ
        return ChatResponse(
            session_id=session.session_id,
            current_step=ChatStep.DIAGNOSING.value,
            prompt=PromptInfo(
                type="single_choice",
                message=message,
                choices=[
                    {"value": "yes", "label": "è§£æ±ºã—ã¾ã—ãŸ"},
                    {"value": "no", "label": "è§£æ±ºã—ã¦ã„ã¾ã›ã‚“"},
                    {"value": "book", "label": "äºˆç´„ã—ãŸã„"},
                ],
            ),
            rag_sources=rag_sources,
        )

    if action == "clarify_term":
        session.conversation_history.append({"role": "assistant", "content": message})
        session.last_questions.append(message)
        # A) ã€Œã‚ã‹ã‚‰ãªã„ã€ã€Œè‡ªç”±å…¥åŠ›ã€ã‚’æœ«å°¾ã«å¿…ãšè¿½åŠ 
        prompt_choices = _append_default_choices(choices)
        return ChatResponse(
            session_id=session.session_id,
            current_step=ChatStep.DIAGNOSING.value,
            prompt=PromptInfo(
                type="single_choice",
                message=message,
                choices=prompt_choices,
            ),
        )

    # ---------------------------------------------------------------
    # Task 3: å€™è£œæç¤º â€” candidates_just_triggered ã‹ã¤ choices ãŒæƒã£ã¦ã„ã‚Œã°
    #          diagnosis_candidates ã¨ã—ã¦è¿”ã™ï¼ˆTip 1: ãƒ©ãƒ™ãƒ«è£œå¼·ï¼‰
    # ---------------------------------------------------------------
    if candidates_just_triggered and choices and len(choices) >= 4:
        prompt_choices = [{"value": c, "label": _enrich_candidate_label(c)} for c in choices]
        session.conversation_history.append({"role": "assistant", "content": message})
        return ChatResponse(
            session_id=session.session_id,
            current_step=ChatStep.DIAGNOSING.value,
            prompt=PromptInfo(
                type="diagnosis_candidates",
                message=message,
                choices=prompt_choices,
            ),
        )

    # ---------------------------------------------------------------
    # 9. ask_question â€” duplicate guard
    # ---------------------------------------------------------------
    if _is_duplicate_question(message, session.last_questions):
        logger.warning(f"Duplicate question detected, replacing: {message!r}")
        message = _pick_fallback_question(session)

    # A) ã€Œã‚ã‹ã‚‰ãªã„ã€ã€Œè‡ªç”±å…¥åŠ›ã€ã‚’æœ«å°¾ã«å¿…ãšè¿½åŠ 
    choices_for_prompt = _append_default_choices(choices)

    session.last_questions.append(message)
    session.conversation_history.append({"role": "assistant", "content": message})
    return ChatResponse(
        session_id=session.session_id,
        current_step=ChatStep.DIAGNOSING.value,
        prompt=PromptInfo(
            type="single_choice",
            message=message,
            choices=choices_for_prompt,
        ),
    )

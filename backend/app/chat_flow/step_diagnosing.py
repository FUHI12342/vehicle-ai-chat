import json
import logging
import re

from app.models.session import SessionState, ChatStep
from app.models.chat import ChatRequest, ChatResponse, PromptInfo, RAGSource
from app.llm.registry import provider_registry
from app.llm.prompts import SYSTEM_PROMPT, DIAGNOSTIC_PROMPT, CONVERSATION_SUMMARY_PROMPT
from app.llm.schemas import DIAGNOSTIC_SCHEMA
from app.services.rag_service import rag_service
from app.services.urgency_assessor import keyword_urgency_check
from app.data.diagnostic_config_loader import get_candidate_hints

logger = logging.getLogger(__name__)

# Task 2: å¾…ã¡ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ¤œå‡ºãƒ‘ã‚¿ãƒ¼ãƒ³
_WAITING_PATTERN = re.compile(r"ã¾ã¨ã‚|æ•´ç†|ãŠå¾…ã¡|ç¢ºèª.{0,5}ã•ã›|å°‘ã€…", re.UNICODE)

# Tip 1: å€™è£œãƒ©ãƒ™ãƒ«è£œåŠ©è¾æ›¸ï¼ˆYAML ã‹ã‚‰èª­ã¿è¾¼ã¿ï¼‰
_CANDIDATE_HINTS = get_candidate_hints()


def _enrich_candidate_label(label: str) -> str:
    """çŸ­ã™ãã‚‹å€™è£œãƒ©ãƒ™ãƒ«ã‚’è£œåŠ©è¾æ›¸ã§èª¬æ˜ä»˜ãã«å¤‰æ›ã™ã‚‹ã€‚"""
    s = label.strip()
    # æ—¢ã«æ‹¬å¼§ä»˜ãã‹ååˆ†ãªé•·ã•ãªã‚‰å¤‰æ›ä¸è¦
    if "ï¼ˆ" in s or "(" in s or len(s) >= 12:
        return s
    for key, hint in _CANDIDATE_HINTS.items():
        if key in s:
            return hint
    return s


# A) ask_question / clarify_term ã®æœ«å°¾ã«å¿…ãšè¿½åŠ ã™ã‚‹ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé¸æŠè‚¢
_DEFAULT_TAIL: list[dict] = [
    {"value": "dont_know", "label": "ã‚ã‹ã‚‰ãªã„"},
    {"value": "free_input", "label": "âœï¸ è‡ªç”±å…¥åŠ›"},
]


def _append_default_choices(choices: list[str] | None) -> list[dict]:
    """LLM ãŒè¿”ã—ãŸ choices ã«ã€Œã‚ã‹ã‚‰ãªã„ã€ã€Œè‡ªç”±å…¥åŠ›ã€ã‚’æœ«å°¾è¿½åŠ ã™ã‚‹ï¼ˆé‡è¤‡é™¤å¤–ï¼‰ã€‚"""
    result: list[dict] = []
    if choices:
        result = [{"value": c, "label": _enrich_candidate_label(c)} for c in choices]
    existing_values = {d["value"] for d in result}
    for tail in _DEFAULT_TAIL:
        if tail["value"] not in existing_values:
            result.append(tail)
    return result


def _is_waiting_message(msg: str) -> bool:
    """True if message looks like a 'please wait' transition, not a real question."""
    if "ï¼Ÿ" in msg or "?" in msg:
        return False
    return bool(_WAITING_PATTERN.search(msg))


def _normalize_question(text: str) -> str:
    """Normalize a question for duplicate comparison."""
    text = re.sub(r"[ï¼Ÿ?ã€‚ã€ï¼!.,\sã€€]+", "", text)
    return text.lower()


def _is_duplicate_question(message: str, last_questions: list[str]) -> bool:
    """Check if the LLM question is semantically a duplicate of a recent one."""
    norm_new = _normalize_question(message)
    if not norm_new:
        return False
    for prev in last_questions:
        norm_prev = _normalize_question(prev)
        if not norm_prev:
            continue
        if norm_new == norm_prev:
            return True
        shorter, longer = sorted([norm_new, norm_prev], key=len)
        if len(shorter) >= 4 and shorter in longer:
            return True
    return False


# ---------------------------------------------------------------------------
# ãƒˆãƒ”ãƒƒã‚¯é–¢é€£æ€§ã‚¬ãƒ¼ãƒ‰
# ---------------------------------------------------------------------------
# ç—‡çŠ¶ã«å«ã¾ã‚Œãªã„é™ã‚Šãƒ–ãƒ­ãƒƒã‚¯ã™ã¹ããƒˆãƒ”ãƒƒã‚¯ã¨ãã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
_GUARDED_TOPICS: dict[str, list[str]] = {
    "éŸ³": ["éŸ³", "ã‚µã‚¦ãƒ³ãƒ‰", "é³´", "ã‚­ãƒ¼", "ã‚´ãƒª", "ã‚«ã‚¿", "ã‚¬ã‚¿", "ã‚®ãƒ¼", "ç•°éŸ³"],
    "æŒ¯å‹•": ["æŒ¯å‹•", "ãƒ–ãƒ«ãƒ–ãƒ«", "ã‚¬ã‚¯ã‚¬ã‚¯", "æºã‚Œ"],
    "è‡­ã„": ["è‡­", "åŒ‚", "ã«ãŠã„", "ã‚¹ãƒ¡ãƒ«"],
    "ç…™": ["ç…™", "ç™½ç…™", "é»’ç…™"],
}


def _is_irrelevant_topic(topic: str, symptom_text: str, conversation_history: list[dict]) -> bool:
    """question_topic ãŒãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç—‡çŠ¶ãƒ»ä¼šè©±ã«ç„¡é–¢ä¿‚ã‹ã©ã†ã‹åˆ¤å®šã™ã‚‹ã€‚

    ã‚¬ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã«ã‚ã‚‹ãƒˆãƒ”ãƒƒã‚¯ã«ã¤ã„ã¦ã€ç—‡çŠ¶ãƒ†ã‚­ã‚¹ãƒˆã¨ä¼šè©±å±¥æ­´ã«
    é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒä¸€åˆ‡å«ã¾ã‚Œã¦ã„ãªã„å ´åˆã« True ã‚’è¿”ã™ã€‚
    ã‚¬ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã«ãªã„ãƒˆãƒ”ãƒƒã‚¯ã¯å¸¸ã« Falseï¼ˆè¨±å¯ï¼‰ã€‚
    """
    # å…¨ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆã—ã¦æ¤œç´¢å¯¾è±¡ã«ã™ã‚‹
    all_text = symptom_text
    for entry in conversation_history:
        if entry["role"] == "user":
            all_text += " " + entry["content"]

    for guarded_name, keywords in _GUARDED_TOPICS.items():
        # topic ãŒã“ã®ã‚¬ãƒ¼ãƒ‰ã‚«ãƒ†ã‚´ãƒªã«è©²å½“ã™ã‚‹ã‹
        if any(kw in topic for kw in keywords) or guarded_name in topic:
            # ç—‡çŠ¶ãƒ†ã‚­ã‚¹ãƒˆ+ä¼šè©±ã«ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒ1ã¤ã§ã‚‚ã‚ã‚Œã°é–¢é€£ã‚ã‚Š
            if any(kw in all_text for kw in keywords):
                return False  # é–¢é€£ã‚ã‚Š â†’ ãƒ–ãƒ­ãƒƒã‚¯ã—ãªã„
            return True  # é–¢é€£ãªã— â†’ ãƒ–ãƒ­ãƒƒã‚¯
    return False  # ã‚¬ãƒ¼ãƒ‰å¯¾è±¡å¤– â†’ è¨±å¯


# ---------------------------------------------------------------------------
# RAGé§†å‹•å‹ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
# ---------------------------------------------------------------------------

def _build_recent_turns(session: SessionState, n: int = 4) -> str:
    """ç›´è¿‘Nä»¶ã®ã‚„ã‚Šå–ã‚Šã®ã¿ãƒ†ã‚­ã‚¹ãƒˆåŒ–ã™ã‚‹ã€‚"""
    history = session.conversation_history
    recent = history[-n:] if len(history) > n else history
    lines = []
    for entry in recent:
        role = "ãƒ¦ãƒ¼ã‚¶ãƒ¼" if entry["role"] == "user" else "ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ"
        lines.append(f"{role}: {entry['content']}")
    return "\n".join(lines) if lines else "(åˆå›å…¥åŠ›)"


def _build_additional_instructions(session: SessionState, user_input: str, candidates_just_triggered: bool) -> str:
    """æ¡ä»¶ä»˜ãæŒ‡ç¤ºã‚’ä¸€æ‹¬æ§‹ç¯‰ã—ã¦è¿”ã™ã€‚"""
    parts: list[str] = []

    # æ”¹å–„C: Spec hint injection
    if session.spec_hint:
        parts.append(
            "\n\nã€å‚è€ƒã€‘ã“ã®ç—‡çŠ¶ã¯ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã«ä»•æ§˜ã¨ã—ã¦è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚"
            "ãƒãƒ‹ãƒ¥ã‚¢ãƒ«é–¢é€£æƒ…å ±ã‚’ç¢ºèªã—ã€ä»•æ§˜ã«è©²å½“ã™ã‚‹å ´åˆã¯ action: \"spec_answer\" ã‚’å„ªå…ˆã—ã¦ãã ã•ã„ã€‚"
        )

    # Force provide_answer if max turns reached
    if session.diagnostic_turn >= session.max_diagnostic_turns:
        parts.append(
            "\n\nã€é‡è¦ã€‘å•è¨ºå›æ•°ã®ä¸Šé™ã«é”ã—ã¾ã—ãŸã€‚ã“ã‚Œã¾ã§ã®æƒ…å ±ã‚’ã‚‚ã¨ã« action: \"provide_answer\" ã§å›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚"
        )

    # è§£æ±ºç­–æç¤ºãƒˆãƒªã‚¬ãƒ¼
    if candidates_just_triggered or (session.candidates_shown and session.solutions_tried == 0):
        parts.append(
            "\n\nã€é‡è¦ã€‘ã“ã‚Œã¾ã§ã®æƒ…å ±ã‹ã‚‰ã€æœ€ã‚‚å¯èƒ½æ€§ã®é«˜ã„åŸå› ã‚’1ã¤ç‰¹å®šã—ã€"
            "ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè‡ªåˆ†ã§è©¦ã›ã‚‹å…·ä½“çš„ãªå¯¾å‡¦æ‰‹é †ã‚’ action: \"provide_answer\" ã§æç¤ºã—ã¦ãã ã•ã„ã€‚"
            "æ‰‹é †ã¯ç•ªå·ä»˜ãã§ã€ç´ äººã§ã‚‚ã§ãã‚‹å†…å®¹ã«ã—ã¦ãã ã•ã„ã€‚"
        )
    elif session.solutions_tried > 0:
        parts.append(
            f"\n\nã€é‡è¦ã€‘å‰å›æç¤ºã—ãŸè§£æ±ºç­–ã§ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å•é¡ŒãŒè§£æ±ºã—ã¾ã›ã‚“ã§ã—ãŸï¼ˆ{session.solutions_tried}å›ç›®ï¼‰ã€‚"
            "æ¬¡ã«å¯èƒ½æ€§ã®é«˜ã„åˆ¥ã®åŸå› ã¨å¯¾å‡¦æ³•ã‚’ action: \"provide_answer\" ã§æç¤ºã—ã¦ãã ã•ã„ã€‚"
            "å‰å›ã¨ç•°ãªã‚‹åŸå› ãƒ»å¯¾å‡¦æ³•ã‚’æç¤ºã—ã¦ãã ã•ã„ã€‚"
        )

    return "".join(parts)


async def _maybe_summarize(session: SessionState, provider) -> None:
    """diagnostic_turn ãŒ3ã®å€æ•°ã‹ã¤ >= 3 ã®ã¨ãã€ä¼šè©±ã‚’è¦ç´„ã—ã¦ conversation_summary ã‚’æ›´æ–°ã€‚"""
    if session.diagnostic_turn < 3 or session.diagnostic_turn % 3 != 0:
        return

    # è¦ç´„å¯¾è±¡: conversation_history å…¨ä½“
    lines = []
    for entry in session.conversation_history:
        role = "ãƒ¦ãƒ¼ã‚¶ãƒ¼" if entry["role"] == "user" else "ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ"
        lines.append(f"{role}: {entry['content']}")
    conversation_text = "\n".join(lines)

    summary_prompt = CONVERSATION_SUMMARY_PROMPT.format(conversation_text=conversation_text)

    try:
        response = await provider.chat(
            messages=[
                {"role": "user", "content": summary_prompt},
            ],
            temperature=0.1,
        )
        session.conversation_summary = response.content.strip()
        logger.info(f"Conversation summary updated (turn {session.diagnostic_turn})")
    except Exception as e:
        logger.warning(f"Conversation summary failed: {e}")


async def _llm_call(provider, diagnostic_prompt: str) -> dict:
    """Call LLM with DIAGNOSTIC_SCHEMA and return parsed JSON."""
    response = await provider.chat(
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": diagnostic_prompt},
        ],
        temperature=0.3,
        response_format={"type": "json_schema", "json_schema": DIAGNOSTIC_SCHEMA},
    )
    return json.loads(response.content)


async def handle_diagnosing(session: SessionState, request: ChatRequest) -> ChatResponse:
    user_input = (request.message or "").strip()

    # Handle "resolved" action from provide_answer step
    if request.action == "resolved":
        if request.action_value == "yes":
            session.current_step = ChatStep.DONE
            return ChatResponse(
                session_id=session.session_id,
                current_step=ChatStep.DONE.value,
                prompt=PromptInfo(
                    type="text",
                    message="ãŠå½¹ã«ç«‹ã¦ã¦è‰¯ã‹ã£ãŸã§ã™ï¼ä»–ã«ã”è³ªå•ãŒã‚ã‚Œã°ã€æ–°ã—ã„å•è¨ºã‚’é–‹å§‹ã—ã¦ãã ã•ã„ã€‚\nå®‰å…¨é‹è»¢ã‚’ãŠé¡˜ã„ã„ãŸã—ã¾ã™ã€‚",
                ),
            )
        elif request.action_value == "no":
            session.solutions_tried += 1
            # 3å›è§£æ±ºç­–ã‚’è©¦ã—ã¦ã‚‚è§£æ±ºã—ãªã„å ´åˆ â†’ å°‚é–€å®¶ã¸
            if session.solutions_tried >= 3:
                session.current_step = ChatStep.URGENCY_CHECK
                from app.chat_flow.step_urgency import handle_urgency_check
                return await handle_urgency_check(session, request)
            # ã¾ã åˆ¥ã®è§£æ±ºç­–ã‚’è©¦ã™ â†’ DIAGNOSING ã«ç•™ã¾ã‚Šæ¬¡ã®ç­–ã‚’æç¤º
            request.message = "è§£æ±ºã—ã¾ã›ã‚“ã§ã—ãŸã€‚ä»–ã®åŸå› ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚"
            return await handle_diagnosing(session, request)
        elif request.action_value == "book":
            # ã€Œç‚¹æ¤œã‚’äºˆç´„ã™ã‚‹ã€ã‚’ç›´æ¥é¸æŠ
            session.current_step = ChatStep.RESERVATION
            from app.chat_flow.step_reservation import handle_reservation
            return await handle_reservation(session, request)
        else:
            logger.warning(f"Unexpected resolved value: {request.action_value!r}")
            return ChatResponse(
                session_id=session.session_id,
                current_step=ChatStep.DIAGNOSING.value,
                prompt=PromptInfo(type="text", message="ç—‡çŠ¶ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚"),
            )

    if not user_input:
        return ChatResponse(
            session_id=session.session_id,
            current_step=ChatStep.DIAGNOSING.value,
            prompt=PromptInfo(
                type="text",
                message="ç—‡çŠ¶ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚",
            ),
        )

    # ---------------------------------------------------------------
    # 1. Save user input + diagnostic_turn++
    # ---------------------------------------------------------------
    session.collected_symptoms.append(user_input)
    session.conversation_history.append({"role": "user", "content": user_input})
    session.diagnostic_turn += 1

    # 2. Keyword-based urgency check (fast path for critical)
    all_symptoms = " ".join(session.collected_symptoms)
    keyword_result = keyword_urgency_check(all_symptoms)
    if keyword_result and keyword_result["level"] == "critical":
        session.urgency_level = "critical"
        session.can_drive = False
        session.current_step = ChatStep.RESERVATION
        from app.chat_flow.step_reservation import handle_reservation
        return await handle_reservation(session, request)

    # 3. RAG query: use rewritten_query if available, otherwise all_symptoms
    rag_query = session.rewritten_query if session.rewritten_query else all_symptoms
    rag_context = "é–¢é€£ã™ã‚‹ãƒãƒ‹ãƒ¥ã‚¢ãƒ«æƒ…å ±ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚"
    rag_sources: list[RAGSource] = []
    try:
        results = await rag_service.query(
            symptom=rag_query,
            vehicle_id=session.vehicle_id,
            make=session.vehicle_make or "",
            model=session.vehicle_model or "",
            year=session.vehicle_year or 0,
        )
        if results["sources"]:
            rag_context = results["answer"]
            rag_sources = [
                RAGSource(
                    content=s["content"],
                    page=s["page"],
                    section=s["section"],
                    score=s["score"],
                )
                for s in results["sources"]
            ]
    except Exception as e:
        logger.warning(f"RAG query failed: {e}")

    # 4. Maybe summarize conversation (every 3 turns)
    provider = provider_registry.get_active()
    if not provider or not provider.is_configured():
        return ChatResponse(
            session_id=session.session_id,
            current_step=ChatStep.DIAGNOSING.value,
            prompt=PromptInfo(
                type="text",
                message="LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚",
            ),
        )

    await _maybe_summarize(session, provider)

    # 5. Candidate trigger: confidence >= 0.7 OR turn >= 4 (fallback)
    candidates_just_triggered = False
    if not session.candidates_shown:
        if session.last_confidence >= 0.7 or session.diagnostic_turn >= 4:
            session.candidates_shown = True
            candidates_just_triggered = True

    # 6. Build prompt
    recent_turns = _build_recent_turns(session)
    additional_instructions = _build_additional_instructions(session, user_input, candidates_just_triggered)

    diagnostic_prompt = DIAGNOSTIC_PROMPT.format(
        make=session.vehicle_make or "ä¸æ˜",
        model=session.vehicle_model or "ä¸æ˜",
        year=session.vehicle_year or "ä¸æ˜",
        original_symptom=session.symptom_text or all_symptoms,
        conversation_summary=session.conversation_summary or "(ãªã—)",
        recent_turns=recent_turns,
        rag_context=rag_context,
        additional_instructions=additional_instructions,
    )

    # 7. Call LLM
    try:
        result = await _llm_call(provider, diagnostic_prompt)
    except (json.JSONDecodeError, Exception) as e:
        logger.error(f"LLM diagnostic call failed: {e}")
        fallback_msg = "ä»–ã«æ°—ã«ãªã‚‹ç—‡çŠ¶ã‚„çŠ¶æ³ãŒã‚ã‚Œã°æ•™ãˆã¦ãã ã•ã„ã€‚"
        session.last_questions.append(fallback_msg)
        session.conversation_history.append({"role": "assistant", "content": fallback_msg})
        return ChatResponse(
            session_id=session.session_id,
            current_step=ChatStep.DIAGNOSING.value,
            prompt=PromptInfo(type="text", message=fallback_msg),
        )

    action = result.get("action", "ask_question")
    message = result.get("message", "")
    urgency_flag = result.get("urgency_flag", "none")
    reasoning = result.get("reasoning", "")
    choices = result.get("choices")
    can_drive_llm: bool | None = result.get("can_drive")

    # 8. Save rewritten_query and confidence
    session.rewritten_query = result.get("rewritten_query", "")
    session.last_confidence = result.get("confidence_to_answer", 0.0)
    question_topic = result.get("question_topic", "")

    logger.info(
        f"Diagnostic action={action}, urgency={urgency_flag}, "
        f"confidence={session.last_confidence:.2f}, topic={question_topic!r}, reasoning={reasoning}"
    )

    # 8b. Topic relevance guard: reject questions on topics absent from symptom text
    if action == "ask_question" and question_topic:
        symptom_text = (session.symptom_text or "") + " " + " ".join(session.collected_symptoms)
        if _is_irrelevant_topic(question_topic, symptom_text, session.conversation_history):
            logger.warning(
                f"Irrelevant topic blocked: topic={question_topic!r}, symptom={session.symptom_text!r}"
            )
            # Force a re-call with explicit instruction
            regen_prompt = (
                diagnostic_prompt
                + f"\n\nã€é‡è¦ã€‘ã€Œ{question_topic}ã€ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç—‡çŠ¶ã¨ç„¡é–¢ä¿‚ã§ã™ã€‚"
                "ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå ±å‘Šã—ãŸç—‡çŠ¶ã®æ–‡é¢ã«å«ã¾ã‚Œã‚‹ãƒˆãƒ”ãƒƒã‚¯ã ã‘ã«åŸºã¥ã„ã¦è³ªå•ã—ã¦ãã ã•ã„ã€‚"
                "ç—‡çŠ¶ã®åŸå› ã‚’çµã‚Šè¾¼ã‚€ãŸã‚ã«ã€æ“ä½œã®çŠ¶æ³ãƒ»æ¡ä»¶ãƒ»å†ç¾æ€§ãªã©ã€ç—‡çŠ¶ã«ç›´çµã™ã‚‹è³ªå•ã‚’ã—ã¦ãã ã•ã„ã€‚"
            )
            try:
                result = await _llm_call(provider, regen_prompt)
                action = result.get("action", "ask_question")
                message = result.get("message", message)
                urgency_flag = result.get("urgency_flag", urgency_flag)
                choices = result.get("choices")
                can_drive_llm = result.get("can_drive", can_drive_llm)
                session.rewritten_query = result.get("rewritten_query", session.rewritten_query)
                session.last_confidence = result.get("confidence_to_answer", session.last_confidence)
                question_topic = result.get("question_topic", "")
            except Exception as e:
                logger.warning(f"Topic guard re-call failed: {e}")

    # ---------------------------------------------------------------
    # 9. å¾…ã¡ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ¤œå‡º â†’ ãƒªãƒˆãƒ©ã‚¤ã—ã¦ provide_answer ã‚’å–å¾—
    # ---------------------------------------------------------------
    if action == "ask_question" and _is_waiting_message(message):
        logger.warning(f"Waiting message detected, retrying: {message!r}")
        retry_prompt = (
            diagnostic_prompt
            + "\n\nã€é‡è¦ã€‘ã€Œã¾ã¨ã‚ã¾ã™ã€ã€Œæ•´ç†ã—ã¾ã™ã€ãªã©ã®å¾…æ©Ÿãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯å‡ºã•ãªã„ã§ãã ã•ã„ã€‚"
            "ä»Šã™ã action: \"provide_answer\" ã§è¨ºæ–­çµæœã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚"
        )
        try:
            result = await _llm_call(provider, retry_prompt)
            action = result.get("action", "provide_answer")
            message = result.get("message", message)
            urgency_flag = result.get("urgency_flag", urgency_flag)
            choices = result.get("choices")
            can_drive_llm = result.get("can_drive", can_drive_llm)
            session.rewritten_query = result.get("rewritten_query", session.rewritten_query)
            session.last_confidence = result.get("confidence_to_answer", session.last_confidence)
        except Exception as e:
            logger.warning(f"Retry LLM call failed: {e}")
            action = "provide_answer"

    # 10. Check urgency_flag from LLM
    if urgency_flag in ("high", "critical"):
        session.urgency_level = urgency_flag
        session.can_drive = can_drive_llm if can_drive_llm is not None else (urgency_flag != "critical")
        if urgency_flag == "critical":
            session.current_step = ChatStep.RESERVATION
            session.conversation_history.append({"role": "assistant", "content": message})
            from app.chat_flow.step_reservation import handle_reservation
            return await handle_reservation(session, request)

    # 11. Dispatch based on action
    if action == "escalate":
        session.urgency_level = urgency_flag if urgency_flag in ("high", "critical") else "high"
        session.can_drive = session.urgency_level != "critical"
        session.conversation_history.append({"role": "assistant", "content": message})
        session.current_step = ChatStep.RESERVATION
        from app.chat_flow.step_reservation import handle_reservation
        return await handle_reservation(session, request)

    # æ”¹å–„C: spec_answer â€” redirect to SPEC_CHECK flow
    if action == "spec_answer":
        session.spec_check_shown = True
        session.current_step = ChatStep.SPEC_CHECK
        session.conversation_history.append({"role": "assistant", "content": message})

        spec_message = f"ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã‚’ç¢ºèªã—ãŸã¨ã“ã‚ã€ã“ã‚Œã¯ä»•æ§˜ï¼ˆæ­£å¸¸ãªå‹•ä½œï¼‰ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\n\n{message}"
        spec_message += "\n\nã“ã®èª¬æ˜ã§ç–‘å•ã¯è§£æ±ºã—ã¾ã—ãŸã‹ï¼Ÿ"

        spec_choices = [
            {"value": "resolved", "label": "è§£æ±ºã—ã¾ã—ãŸ"},
            {"value": "not_resolved", "label": "è§£æ±ºã—ã¦ã„ã¾ã›ã‚“"},
            {"value": "already_tried", "label": "ãã‚Œã¯è©¦ã—ã¾ã—ãŸ / çŸ¥ã£ã¦ã„ã¾ã™"},
        ]
        return ChatResponse(
            session_id=session.session_id,
            current_step=ChatStep.SPEC_CHECK.value,
            prompt=PromptInfo(
                type="single_choice",
                message=spec_message,
                choices=spec_choices,
            ),
            rag_sources=rag_sources,
        )

    if action == "provide_answer":
        session.rag_answer = message
        session.conversation_history.append({"role": "assistant", "content": message})

        # C) high/critical â†’ å¼·ã„è­¦å‘Š + äºˆç´„å°ç·šï¼ˆreservation_choiceï¼‰
        if urgency_flag in ("high", "critical"):
            effective_can_drive = can_drive_llm if can_drive_llm is True else False
            session.urgency_level = urgency_flag
            session.can_drive = effective_can_drive
            session.current_step = ChatStep.RESERVATION

            if not effective_can_drive:
                warning = (
                    "ğŸš¨ã€è‡ªèµ°ç¦æ­¢ã€‘ã™ãã«é‹è»¢ã‚’ä¸­æ­¢ã—ã€å®‰å…¨ãªå ´æ‰€ã«åœè»Šã—ã¦ãã ã•ã„ã€‚\n\n"
                    f"{message}\n\n"
                    "ãƒ­ãƒ¼ãƒ‰ã‚µãƒ¼ãƒ“ã‚¹ã¸ã®é€£çµ¡ã‚’å¼·ããŠå‹§ã‚ã—ã¾ã™ã€‚"
                )
                reservation_choices = [
                    {"value": "dispatch", "label": "ãƒ­ãƒ¼ãƒ‰ã‚µãƒ¼ãƒ“ã‚¹ã‚’å‘¼ã¶"},
                    {"value": "skip", "label": "ä»Šã¯äºˆç´„ã—ãªã„"},
                ]
            else:
                warning = (
                    "âš ï¸ã€æ—©æ€¥ãªç‚¹æ¤œæ¨å¥¨ã€‘ç„¡ç†ãªé‹è»¢ã¯é¿ã‘ã¦ãã ã•ã„ã€‚\n\n"
                    f"{message}\n\n"
                    "æ—©æ€¥ã«ãƒ‡ã‚£ãƒ¼ãƒ©ãƒ¼ã¾ãŸã¯æ•´å‚™å·¥å ´ã§ã®ç‚¹æ¤œã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚"
                )
                reservation_choices = [
                    {"value": "dispatch", "label": "ãƒ­ãƒ¼ãƒ‰ã‚µãƒ¼ãƒ“ã‚¹ã‚’å‘¼ã¶"},
                    {"value": "visit", "label": "ãƒ‡ã‚£ãƒ¼ãƒ©ãƒ¼ã«æŒã¡è¾¼ã‚€"},
                    {"value": "skip", "label": "ä»Šã¯äºˆç´„ã—ãªã„"},
                ]

            return ChatResponse(
                session_id=session.session_id,
                current_step=ChatStep.RESERVATION.value,
                prompt=PromptInfo(
                    type="reservation_choice",
                    message=warning,
                    choices=reservation_choices,
                    booking_type=session.booking_type,
                ),
                rag_sources=rag_sources,
            )

        # low/medium/none â†’ è§£æ±ºç¢ºèª + äºˆç´„ã¸ã®ã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆ
        return ChatResponse(
            session_id=session.session_id,
            current_step=ChatStep.DIAGNOSING.value,
            prompt=PromptInfo(
                type="single_choice",
                message=message,
                choices=[
                    {"value": "yes", "label": "è§£æ±ºã—ã¾ã—ãŸ"},
                    {"value": "no", "label": "è§£æ±ºã—ã¦ã„ã¾ã›ã‚“"},
                    {"value": "book", "label": "äºˆç´„ã—ãŸã„"},
                ],
            ),
            rag_sources=rag_sources,
        )

    if action == "clarify_term":
        session.conversation_history.append({"role": "assistant", "content": message})
        session.last_questions.append(message)
        prompt_choices = _append_default_choices(choices)
        return ChatResponse(
            session_id=session.session_id,
            current_step=ChatStep.DIAGNOSING.value,
            prompt=PromptInfo(
                type="single_choice",
                message=message,
                choices=prompt_choices,
            ),
        )

    # ---------------------------------------------------------------
    # 12. ask_question â€” duplicate guard (lightweight)
    # ---------------------------------------------------------------
    if _is_duplicate_question(message, session.last_questions):
        logger.warning(f"Duplicate question detected, replacing: {message!r}")
        message = "ä»–ã«æ°—ã«ãªã‚‹ç—‡çŠ¶ã‚„çŠ¶æ³ãŒã‚ã‚Œã°æ•™ãˆã¦ãã ã•ã„ã€‚"

    # A) ã€Œã‚ã‹ã‚‰ãªã„ã€ã€Œè‡ªç”±å…¥åŠ›ã€ã‚’æœ«å°¾ã«å¿…ãšè¿½åŠ 
    choices_for_prompt = _append_default_choices(choices)

    session.last_questions.append(message)
    session.conversation_history.append({"role": "assistant", "content": message})
    return ChatResponse(
        session_id=session.session_id,
        current_step=ChatStep.DIAGNOSING.value,
        prompt=PromptInfo(
            type="single_choice",
            message=message,
            choices=choices_for_prompt,
        ),
    )
